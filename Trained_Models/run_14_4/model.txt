
#### First model 

global test_log =[[],[],[],[],[],[],[]]
global training_log = [[],[],[],[],[],[],[]]

opt = Adam(0.0001)
model_name = "run_14_4"
model = Chain(
    #First convolution
    BatchNorm(1),
    Conv((3,3), 1=>16,leakyrelu),
    x-> maxpool(x,(2,2),stride = 2 ),
    #Second convolution 
    Conv((3,3), 16=>16,leakyrelu),
    x-> maxpool(x,(2,2),stride = 2 ),

    #Third convolution 
    Parallel(hcat,
        Conv((1,1), 16=>4,elu),
    Chain(
        Conv((1,1), 16=>8,elu;pad = 0),
        Conv((3,3), 8=>4,elu; pad= 1)),
    Chain(
        Conv((1,1), 16=>8,elu ; pad = 0),
        Conv((5,5) ,8=>4,elu, pad = 2)),
    Chain(
        x->maxpool(x,(3,3),stride=1,pad=1),
        Conv((1,1),16=>4,elu))),
    
    x->reshape(x,(size(x,1),size(x,1),size(x,3)*4,size(x,4))),
    x-> maxpool(x,(2,2),stride = 2 ),

    #Fourth convolution
    Conv((3,3), 16=>16,leakyrelu),
    x-> maxpool(x,(2,2),stride = 2 ),

    #Fifth convolution 
    SkipConnection(Chain(Conv((3,3), 16=>16,pad=(1,1), leakyrelu),Conv((3,3), 16=>16,pad=(1,1), leakyrelu)),(mx,x)->mx.+x),
    #AlphaDropout(0.5),
    Flux.GlobalMeanPool(),
    Flux.flatten,
    #Fully connected layer
    Dense(16=>1),
    sigmoid_fast
    #TODO: softmax needs to be added to last dense layer

    )|> gpu 