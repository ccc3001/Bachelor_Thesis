@Article{Walle2023,
  author     = {Matthias Walle and Dominic Eggemann and Penny R. Atkins and Jack J. Kendall and Kerstin Stock and Ralph Müller and Caitlyn J. Collins},
  journal    = {Bone},
  title      = {Motion grading of high-resolution quantitative computed tomography supported by deep convolutional neural networks},
  year       = {2023},
  month      = {jan},
  pages      = {116607},
  volume     = {166},
  doi        = {10.1016/j.bone.2022.116607},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{Zhang2020,
  author     = {Qiang Zhang and Evan Hann and Konrad Werys and Cody Wu and Iulia Popescu and Elena Lukaschuk and Ahmet Barutcu and Vanessa M. Ferreira and Stefan K. Piechnik},
  journal    = {Artificial Intelligence in Medicine},
  title      = {Deep learning with attention supervision for automated motion artefact detection in quality control of cardiac T1-mapping},
  year       = {2020},
  month      = {nov},
  pages      = {101955},
  volume     = {110},
  doi        = {10.1016/j.artmed.2020.101955},
  publisher  = {Elsevier {BV}},
  readstatus = {skimmed},
}

@Article{Lorch2017,
  author     = {Benedikt Lorch and Ghislain Vaillant and Christian Baumgartner and Wenjia Bai and Daniel Rueckert and Andreas Maier},
  journal    = {Journal of Medical Engineering},
  title      = {Automated Detection of Motion Artefacts in {MR} Imaging Using Decision Forests},
  year       = {2017},
  month      = {jun},
  pages      = {1--9},
  volume     = {2017},
  doi        = {10.1155/2017/4501647},
  publisher  = {Hindawi Limited},
  readstatus = {skimmed},
}

@Article{Alzubaidi2021,
  author     = {Laith Alzubaidi and Jinglan Zhang and Amjad J. Humaidi and Ayad Al-Dujaili and Ye Duan and Omran Al-Shamma and J. Santamar{\'{\i}}a and Mohammed A. Fadhel and Muthana Al-Amidie and Laith Farhan},
  journal    = {Journal of Big Data},
  title      = {Review of deep learning: concepts, {CNN} architectures, challenges, applications, future directions},
  year       = {2021},
  month      = {mar},
  number     = {1},
  volume     = {8},
  comment    = {Optimizer selection:
- two major issues in the learning process are learning algorithm selection(optimizer) while the other one is the issue of many enhancements(AdaDelta, Adagrad, and momentum)
- batch gradient descent: the network
parameters are updated merely one time behind considering all training datasets via
the network
-Stochastic Gradient Descent: The parameters are updated at each training sample
Mini-batch Gradient Descent: In this approach, the training samples are partitioned
into several mini-batches
-other methodes are Momentum and Adaptive Moment Estimation
-AlexNet improved its learning ability by its depth and implementing several parameter optimization strategies-> number extraction stages was increased
- ZefNet was the frontier network, which proposed that filters with small sizes
could enhance the CNN performance
-VGG inserted a
layer of the heap of 3 × 3 filters rather than the 5 × 5 and 11 × 11 filters in ZefNet.
-GoogLeNet combines multiple-scale convolutional transformations by employing merge, transform, and split functions for feature extraction
-architecture incorporates filters Page 31 of 74Alzubaidi et al. J Big Data (2021) 8:53
of different sizes (5 × 5, 3 × 3, and 1 × 1) to capture channel information together with
spatial information at diverse ranges of spatial resolution
-proposed the idea of aux-
iliary learners to speed up the rate of convergence
-The novel idea of ResNet is its use of the
bypass pathway concept
-ayer widening is a highly successful method of per-
formance enhancement compared to deepening the residual network.
-slow enlargement in the feature map depth based on the
up-down method
-Xception model
adjusted the original inception block by making it wider and exchanging a single
dimension (3 × 3) followed by a 1 × 1 convolution to reduce computational complex-
ity

-> high resolution network might be a good approach for my thesis (necessary for position-sensitive vision tasks)

- TL as compared to the target dataset. For instance,
enhancing the medical image classification performance of CNN models is achieved
by training the models using the ImageNet dataset, which contains natural images

-TL from different domains does not significantly affect performance on
medical imaging tasks

-ncrease the amount of available data and avoid the overfitting issue, data
augmentation techniques are one possible solution : Flipping, Cropping 
Rotation Translation

- biological data tends to be imbalanced, as negative samples are much more numerous than positive ones, care for this before training because the model might not perform well on small classes 
-in healthcare or similar
applications, the uncertainty scaling is frequently very significant

-paper has greate information about how to tackle the vanishing gradient problem 
-underspecification: It has been shown that small modifica-
tions can force a model towards a completely different solution as well as lead to dif-
ferent predictions in deployment domains

Paper contains many intresting evaluation techniques like :Accuracy Sensitivity Specificity, Precision, F1_Score, J_Score, False Positive Rate FPR, Area Under the ROC Curve},
  doi        = {10.1186/s40537-021-00444-8},
  priority   = {prio1},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{LeCun2015,
  author    = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  journal   = {Nature},
  title     = {Deep learning},
  year      = {2015},
  month     = {may},
  number    = {7553},
  pages     = {436--444},
  volume    = {521},
  doi       = {10.1038/nature14539},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Khan2020,
  author     = {Asifullah Khan and Anabia Sohail and Umme Zahoora and Aqsa Saeed Qureshi},
  journal    = {Artificial Intelligence Review},
  title      = {A survey of the recent architectures of deep convolutional neural networks},
  year       = {2020},
  month      = {apr},
  number     = {8},
  pages      = {5455--5516},
  volume     = {53},
  comment    = {- convolutional layer is composed of a set of convolutional kernels
-The use of pooling operation helps to extract a combination of features, which are invar-
iant to translational shifts and small distortions
-ReLU and its variants are preferred as activation function as they help in
overcoming the vanishing gradient problem
-batch normalization ...
-Dropout introduces regularization within the network by randomly skipping some units or connections with a certain probability

-on page 5464 is an tree showing the developement of cnns which has some intrsting concepts written in it

-GoogleNet introduced a structure called the inception block which uses the  split, transform, and merge concept  to capture spatial information at different scales

-CNNs may suffer from performance degradation, gradient vanishing, or 
problems, which are not caused by overfitting but instead by an increase in the depth

continue on 5483 it might be intreresting},
  doi        = {10.1007/s10462-020-09825-6},
  priority   = {prio2},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Misc{Ruder2016,
  author    = {Ruder, Sebastian},
  title     = {An overview of gradient descent optimization algorithms},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1609.04747},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InProceedings{Zhang2018,
  author    = {Zijun Zhang},
  booktitle = {2018 {IEEE}/{ACM} 26th International Symposium on Quality of Service ({IWQoS})},
  title     = {Improved Adam Optimizer for Deep Neural Networks},
  year      = {2018},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/iwqos.2018.8624183},
}

@Misc{Lin2013,
  author     = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  title      = {Network In Network},
  year       = {2013},
  abstract   = {-Introduces the Network in Network structure to enhance model discriminability for local patches
-we build micro neural networks with
more complex structures to abstract the data within the receptive field
-we are able to uti-
lize global average pooling over feature maps in the classification layer, which is
easier to interpret and less prone to overfitting than traditional fully connected lay-
er},
  comment    = {For Code: https://github.com/ducha-aiki/caffenet-benchmark

-generalized linear model(GLM)
- In NIN, the
GLM is replaced with a ”micro network” structure which is a general nonlinear function approxi-
mator
-a micro network is
introduced within each convolutional layer to compute more abstract features for local patches

-Each pooling layer performs
weighted linear recombination on the input feature maps, which then go through a rectifier linear
unit. The cross channel pooled feature maps are cross channel pooled again and again in the next
layers. This cascaded cross channel parameteric pooling structure allows complex and learnable
interactions of cross channel information
-for classification tasks},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1312.4400},
  keywords   = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio2},
  publisher  = {arXiv},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Mishkin2017,
  author     = {Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas},
  journal    = {Computer Vision and Image Understanding},
  title      = {Systematic evaluation of convolution neural network advances on the Imagenet},
  year       = {2017},
  month      = {aug},
  pages      = {11--19},
  volume     = {161},
  abstract   = {-studies the impact of a range of recent advances in
CNN architectures and learning method},
  comment    = {- ELU + maxout hsow the best performance among non-linearities with speed close to ReLu

- Wide maxout outperforms the rest of the competitors at
a higher computational cost
-Pooling, combined with striding, is a common way to archive a degree of in-
variance together with a reduction of spatial size of feature map

Prediction:  max pooling brings selectivity and in-
variance, while average pooling allows using gradients of all filters, instead of
throwing away 3/4 of information 
- most commonly used learn-
ing rate decay policy is ”reduce learning rate 10x, when validation error stops
decreasing
-Batch Normalization is a recent method that solves the gradient ex-
ploding/vanishing problem and guarantees near-optimal learning regim


Conclusion:
• use ELU non-linearity without batchnorm or ReLU with it.
• apply a learned colorspace transformation of RGB.
• use the linear learning rate decay policy.
• use a sum of the average and max pooling layers.
• use mini-batch size around 128 or 256. If this is too big for your GPU,
decrease the learning rate proportionally to the batch size.
• use fully-connected layers as convolutional and average the predictions for
the final decision.
• when investing in increasing training set size, check if a plateau has not
been reach.
• cleanliness of the data is more important then the size.
• if you cannot increase the input image size, reduce the stride in the con-
sequent layers, it has roughly the same effect.
• if your network has a complex and highly optimized architecture, like e.g.
GoogLeNet, be careful with modifications},
  doi        = {10.1016/j.cviu.2017.05.007},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {read},
}

@InCollection{Bottou2010,
  author    = {L{\'{e}}on Bottou},
  booktitle = {Proceedings of {COMPSTAT}{\textquotesingle}2010},
  publisher = {Physica-Verlag {HD}},
  title     = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  year      = {2010},
  pages     = {177--186},
  doi       = {10.1007/978-3-7908-2604-3_16},
}

@Misc{Simonyan2014,
  author     = {Simonyan, Karen and Zisserman, Andrew},
  title      = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year       = {2014},
  abstract   = {-In this work we investigate the effect of the convolutional network depth on its
accuracy in the large-scale image recognition settin},
  comment    = {- the classification error decreases with the increased ConvNet depth: from
11 layers in A to 19 layers in E
-scale jittering at training time leads to significantly better results than
training on images with fixed smallest side
- It was demonstrated that the representation depth is beneficial for the
classification accuracy},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1409.1556},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio2},
  publisher  = {arXiv},
  ranking    = {rank3},
  readstatus = {read},
}

@Misc{Dauphin2015,
  author    = {Dauphin, Yann N. and de Vries, Harm and Bengio, Yoshua},
  title     = {Equilibrated adaptive learning rates for non-convex optimization},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1502.04390},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Misc{Woo2018,
  author     = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  title      = {CBAM: Convolutional Block Attention Module},
  year       = {2018},
  abstract   = {a simple yet effective attention module for feed-forward convolutional
neural networks},
  comment    = {esearches have mainly investigated three
important factors of networks: depth, width, and cardinality
-The overview of CBAM: The module has two sequential sub-modules:
channel and spatial. The intermediate feature map is adaptively refined through
our module (CBAM) at every convolutional block of deep networks
-An intuitive and simple way of extension is to increase the depth of neural
networks
-naive increase in depth comes to saturation due to the difficulty of gradient
propagation
-introduces  cannel attention and spatial attention model 
-One can seamlessly integrate CBAM in any CNN architectures and jointly train the combined
CBAM-enhanced networks
-CBAM outperforms all the
Convolutional Block Attention Module 7
baselines without bells and whistles, demonstrating the general applicability of
CBAM across different architectures as well as different tasks.
-Using both
attention is critical while the best-combining strategy is Network+ channel + spatial
- Grad Cam shows that the use of CBAM in a network gives it a better better and more precise way of detecting objects 
-improve representation power of CNN networks

-},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1807.06521},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio1},
  publisher  = {arXiv},
  readstatus = {read},
}

@InProceedings{He_2016_CVPR,
  author     = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Deep Residual Learning for Image Recognition},
  year       = {2016},
  month      = {June},
  abstract   = {-present a residual learning framework to ease the training
of networks that are substantially deeper than those used
previously

won the 1st place on the
ILSVRC 2015 classification task},
  comment    = {Is
learning better networks as easy as stacking more layers?

-Multigrid method reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale
-An early practice of training multi-layer perceptrons
(MLPs) is to add a linear layer connected from the network
input to the output
-y = F(x, {Wi}) + Wsx.},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
}

@Misc{Hu2017,
  author     = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  title      = {Squeeze-and-Excitation Networks},
  year       = {2017},
  abstract   = {We further demonstrate
that Squeeze and Excitation blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cos

https://github.com/hujie-frank/SENet},
  comment    = {-we propose a mechanism
that allows the network to perform feature recalibration,
through which it can learn to use global information to
selectively emphasise informative features and suppress less
useful ones
-ResNets demonstrated that it was possible to learn considerably deeper and stronger networks
through the use of identity-based skip connections},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1709.01507},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio3},
  publisher  = {arXiv},
  ranking    = {rank3},
  readstatus = {read},
}

@InCollection{Tan2018,
  author     = {Chuanqi Tan and Fuchun Sun and Tao Kong and Wenchang Zhang and Chao Yang and Chunfang Liu},
  booktitle  = {Artificial Neural Networks and Machine Learning {\textendash} {ICANN} 2018},
  publisher  = {Springer International Publishing},
  title      = {A Survey on Deep Transfer Learning},
  year       = {2018},
  pages      = {270--279},
  abstract   = {Transfer learning relaxes the hypothesis
that the training data must be independent and identically distributed
(i.i.d.) with the test data, which motivates us to use transfer learning
to solve the problem of insufficient training data.

- this paper introduces instance based, mapping based, network based and adversarial based transfer learning approaches},
  comment    = {-Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task.[1] For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing/transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency

- might be intresting if i dont want to train tibia and radius together then i might first train radius and afterwards tibia to imrove its performance 

- First, network was trained
in source domain with large-scale training dataset. Second, partial of network pretrained for source domain are transfer to be a part of new network designed for target domain

-Instances-based: Utilize instances in source domain by appropriate weight

Although there
are different between two domains, partial instances in the source domain can
be utilized by the target domain with appropriate weights

-Mapping-based: Mapping instances from two domains into a new data space with better similarity

Although there are different between two origin domains, they can be more similarly in an elaborate new data space.

-Network-based: Reuse the partial of network pre-trained in the source domain

Neural network is similar to the processing mechanism of the human brain, and it is an iterative and continuous
abstraction process. The front-layers of the network can be treated as a feature
extractor, and the extracted features are versatile

-Adversarial-based: Use adversarial technology to find transferable features that both suitable for two domains

Although there
are different between two domains, partial instances in the source domain can
be utilized by the target domain with appropriate weights},
  doi        = {10.1007/978-3-030-01424-7_27},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
}

@InProceedings{NEURIPS2019_eb1e7832,
  author     = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Transfusion: Understanding Transfer Learning for Medical Imaging},
  year       = {2019},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {32},
  abstract   = {In this paper, we explore properties of transfer learning for medical imaging},
  comment    = {-Despite the immense popularity of transfer learning in medical imaging, there has been little work
studying its precise effects

-learning is typically performed by taking a standard IMAGENET architecture along with its pretrained weights, and then fine-tuning on the target task.k. However, IMAGENET classification and medical image diagnosis have considerable differences
-The basic building block for this family is the
popular sequence of a (2d) convolution, followed by batch normalization [ 13 ] and a relu activation

-surprisingly, transfer offers feature independent benefits to convergence simply through better
weight scaling (ii) using pretrained weights from the lowest two layers/stages has the biggest effect
on convergence
-pretrained weights results in faster convergence

-we find
that transfer learning offers limited performance gains and much smaller architectures can perform
comparably to the standard IMAGENET models},
  priority   = {prio3},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
}

@Article{Herzog2020,
  author     = {Lisa Herzog and Elvis Murina and Oliver Dürr and Susanne Wegener and Beate Sick},
  journal    = {Medical Image Analysis},
  title      = {Integrating uncertainty in deep neural networks for {MRI} based stroke analysis},
  year       = {2020},
  month      = {oct},
  pages      = {101790},
  volume     = {65},
  abstract   = {Deep Learning methods provide point predictions with-
out quantifying the model’s uncertainty

-provide an entire framework to diagnose ischemic stroke patients
incorporating Bayesian uncertainty into the analysis procedure},
  comment    = {-Bayesian approaches are the mainstay to quantify uncertainty
in machine learning
-In many MC dropout applications, a correlation between high
uncertainty estimates and erroneous predictions was found
-dropout layers (with dropout probability
0.3) were inserted before each convolutional and fully connected
layer
-For model fitting, we trained with respect to the binary cross-
entropy loss
-The MC dropout variance (Var) is the variance across the soft-
max probabilities

-removing images resultet in a higher accuracy(i dont really understand it right now )

-(ROC) curves and reports the Area under the curve (AUC) statis-
tic, to assess the power of the respective uncertainty measure
to differentiate between correct and erroneous predictions. For
the sake of simplicity, results of the 3Ch-MC are presented
-1Ch-MC showed similar outcomes
-accuracy improved by about 2% when remov-
ing only 5% of the images. In terms of AUC,
-MC dropout does
not only provide uncertainty information but also improves clas-
sification performance.
-integrating uncertainty in-
formation in deep neural network models is essential to achieve
a high performance for stroke classification
-( epistemic
and aleatoric uncertainty)},
  doi        = {10.1016/j.media.2020.101790},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{Pereyra2017,
  author        = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Łukasz and Hinton, Geoffrey},
  title         = {Regularizing Neural Networks by Penalizing Confident Output Distributions},
  year          = {2017},
  month         = jan,
  abstract      = {-We show that penalizing low entropy output distributions,
which has been shown to improve exploration in reinforcement learning,},
  archiveprefix = {arXiv},
  comment       = {-label smoothing regularization, has been
shown to improve generalization},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1701.06548},
  eprint        = {1701.06548},
  file          = {:http\://arxiv.org/pdf/1701.06548v1:PDF},
  keywords      = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.NE},
  priority      = {prio3},
  publisher     = {arXiv},
  ranking       = {rank4},
  readstatus    = {skimmed},
}

@Article{Han2015,
  author        = {Han, Song and Mao, Huizi and Dally, William J.},
  title         = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  year          = {2015},
  month         = oct,
  abstract      = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1510.00149},
  eprint        = {1510.00149},
  file          = {:http\://arxiv.org/pdf/1510.00149v5:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{DAmour2020,
  author        = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  title         = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  year          = {2020},
  month         = nov,
  abstract      = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2011.03395},
  eprint        = {2011.03395},
  file          = {:http\://arxiv.org/pdf/2011.03395v2:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{pmlr-v28-goodfellow13,
  author     = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  booktitle  = {Proceedings of the 30th International Conference on Machine Learning},
  title      = {Maxout Networks},
  year       = {2013},
  address    = {Atlanta, Georgia, USA},
  editor     = {Dasgupta, Sanjoy and McAllester, David},
  month      = {17--19 Jun},
  number     = {3},
  pages      = {1319--1327},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {28},
  abstract   = {We define a simple new model called
maxout (so named because its output is the
max of a set of inputs, and because it is a nat-
ural companion to dropout)

maxout is a activation function which is especially well suited for training with dropout},
  comment    = {- dropout is known to work well in prac-
tice

Code and hyperparameters available at http://www-etud.iro.umontreal.ca/˜goodfeli/maxout.html

-Dropout is a technique that can be applied to deter-
ministic feedforward architectures that predict an out-
put y given input vector v

-maxout model is simply a feed-forward achitec-
ture, such as a multilayer perceptron or deep convo-
lutional neural network, that uses a new type of ac-
tivation function: the maxout unit.

-, it indicates that
dropout does exact model averaging in deeper archi-
tectures provided that they are locally linear among
the space of inputs to each layer

- maxout performs well is
that it improves the bagging style training phase of
dropout

-t maxout allows training deeper
networks, this greater variance suggests that maxout
better propagates varying information downward to
the lower layers and helps dropout training to better
resemble bagging for the lower-layer parameters

-maxout propagates
variations in the gradient due to different choices of
dropout masks to the lowest layers of a network},
  pdf        = {http://proceedings.mlr.press/v28/goodfellow13.pdf},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v28/goodfellow13.html},
}

@Misc{Mishkin2015,
  author     = {Mishkin, Dmytro and Matas, Jiri},
  title      = {All you need is a good init},
  year       = {2015},
  abstract   = {Layer-sequential unit-variance (LSUV) initialization,a simple method for weight
initialization for deep net learning

The method consists of the two
steps. First, pre-initialize weights of each convolution or inner-product layer with
orthonormal matrices. Second, proceed from the first to the final layer, normaliz-
ing the variance of the output of each layer to be equal to one.},
  comment    = {batch normalization adds a 30% computational
overhead to each iteration


Tolvar ... Its value does not
noticeably influence the performance in a broad range of 0.01 to 0.1

L – convolution or full-
connected layer, WL - its weights, BL - its output blob., Tolvar - variance tolerance, Ti – current
trial, Tmax – max number of trials(usually it ends between 1 and 5 trials so maybeset it to 6 or 7 )

Pre-initialize network with orthonormal matrices as in Saxe et al. (2014)
for each layer L do
while |Var(BL) − 1.0| ≥ Tolvar and (Ti < Tmax) do
do Forward pass with a mini-batch
calculate Var(BL)
WL = WL / √Var(BL)
end while
end for

performes well with maxout RELU and VLReLU},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1511.06422},
  keywords   = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio2},
  publisher  = {arXiv},
  ranking    = {rank4},
  readstatus = {read},
}

@Book{Briggs2000,
  author    = {Briggs, William L.},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {A multigrid tutorial.},
  year      = {2000},
  isbn      = {0898714621},
  pages     = {193},
}

@Misc{Kingma2014,
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  title      = {Adam: A Method for Stochastic Optimization},
  year       = {2014},
  abstract   = {algorithm for first-order gradient-based optimization of
stochastic objective functions, based on adaptive estimates of lower-order mo-
ments
-computationally efficient,
has little memory requirements},
  comment    = {adaptive moment estimation(Adam)

Good default settings for the tested machine learning problems are α = 0.001,
β1 = 0.9, β2 = 0.999 and  = 10−8


Require: α: Stepsize
Require: β1, β2 ∈ [0, 1): Exponential decay rates for the moment estimates
Require: f (θ): Stochastic objective function with parameters θ
Require: θ0: Initial parameter vector
m0 ← 0 (Initialize 1st moment vector)
v0 ← 0 (Initialize 2nd moment vector)
t ← 0 (Initialize timestep)
while θt not converged do
t ← t + 1
gt ← ∇θ ft(θt−1) (Get gradients w.r.t. stochastic objective at timestep t)
mt ← β1 · mt−1 + (1 − β1) · gt (Update biased first moment estimate)
vt ← β2 · vt−1 + (1 − β2) · g2
t (Update biased second raw moment estimate)̂
mt ← mt/(1 − βt
1) (Compute bias-corrected first moment estimate)̂
vt ← vt/(1 − βt
2) (Compute bias-corrected second raw moment estimate)
θt ← θt−1 − α ·̂ mt/(√̂ vt + ) (Update parameters)
end while
return θt (Resulting parameters)



in this paper another function is introduced which is called AdaMax-> a varian of Adam based on the infinity norm

methodes are aimed towards ml problems with large data sets  and or high  dimensionality wich is true in my case},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1412.6980},
  keywords   = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio1},
  publisher  = {arXiv},
  ranking    = {rank4},
  readstatus = {read},
}

@InCollection{He2016,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle = {Computer Vision {\textendash} {ECCV} 2016},
  publisher = {Springer International Publishing},
  title     = {Identity Mappings in Deep Residual Networks},
  year      = {2016},
  pages     = {630--645},
  doi       = {10.1007/978-3-319-46493-0_38},
}

@InProceedings{Goodfellow2014,
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Generative Adversarial Nets},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {27},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
}

@Article{Ioffe2015,
  author        = {Ioffe, Sergey and Szegedy, Christian},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year          = {2015},
  month         = feb,
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  comment       = {-We propose a new mechanism, which we call Batch
Normalization, that takes a step towards reducing internal
covariate shift, and in doing so dramatically accelerates the
training of deep neural nets
-normalize each scalar feature independently, by
making it have zero mean and unit variance
-Simply adding Batch Normalization to a network does not
take full advantage of our method
-Increase learning rate
-Remove Dropout
-Shuffle training examples more thoroughly
-Reduce the L2 weight regularization
-Accelerate the learning rate decay
-Remove Local Response Normalization
-Reduce the photometric distortions},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1502.03167},
  eprint        = {1502.03167},
  file          = {:http\://arxiv.org/pdf/1502.03167v3:PDF},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  publisher     = {arXiv},
  ranking       = {rank4},
  readstatus    = {read},
}

@Article{Sode2011,
  author     = {Miki Sode and Andrew J. Burghardt and Jean-Baptiste Pialat and Thomas M. Link and Sharmila Majumdar},
  journal    = {Bone},
  title      = {Quantitative characterization of subject motion in {HR}-{pQCT} images of the distal radius and tibia},
  year       = {2011},
  month      = {jun},
  number     = {6},
  pages      = {1291--1297},
  volume     = {48},
  abstract   = {propose an objective technique for measuring subject motion based on a
comparison of image similarity measure of the parallelized projections acquired at 0° and
180°},
  comment    = {-propose an objective technique for measuring subject motion based on a
comparison of image similarity measure of the parallelized projections acquired at 0° and
180°

Quantitative Motion estimates (QMEs)
-Two similarity measures, the sum of squared intensity difference
(SSD) and normalized cross correlation (NCC) were examined.},
  doi        = {10.1016/j.bone.2011.03.755},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{Hinton2006,
  author     = {G. E. Hinton and R. R. Salakhutdinov},
  journal    = {Science},
  title      = {Reducing the Dimensionality of Data with Neural Networks},
  year       = {2006},
  month      = {jul},
  number     = {5786},
  pages      = {504--507},
  volume     = {313},
  abstract   = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural
network with a small central layer to reconstruct high-dimensional input vectors},
  comment    = {-principal components analysis (PCA), finds the directions of greatest variance in the
data set and represents each data point by its
coordinates along each of these directions
-With large initial weights,
autoencoders typically find poor local minima
-what is a restricted boltzmann machine 

-After learning one layer of feature de-
tectors, we can treat their activities—when they
are being driven by the data—as data for
learning a second layer of features. The first
layer of feature detectors then become the
visible units for learning the next RBM

-latent semantic analysis
(LSA) a well known document retrival method based on PCA},
  doi        = {10.1126/science.1127647},
  priority   = {prio3},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Rusk2015,
  author    = {Nicole Rusk},
  journal   = {Nature Methods},
  title     = {Deep learning},
  year      = {2015},
  month     = {dec},
  number    = {1},
  pages     = {35--35},
  volume    = {13},
  doi       = {10.1038/nmeth.3707},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Lecun1998,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  journal   = {Proceedings of the {IEEE}},
  title     = {Gradient-based learning applied to document recognition},
  year      = {1998},
  number    = {11},
  pages     = {2278--2324},
  volume    = {86},
  doi       = {10.1109/5.726791},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{pmlr-v9-glorot10a,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  year      = {2010},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  month     = {13--15 May},
  pages     = {249--256},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {9},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  pdf       = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url       = {https://proceedings.mlr.press/v9/glorot10a.html},
}

@Misc{Ghiasi2020,
  author    = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
  title     = {Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation},
  year      = {2020},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2012.07177},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Misc{Saxe2013,
  author    = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  title     = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1312.6120},
  keywords  = {Neural and Evolutionary Computing (cs.NE), Disordered Systems and Neural Networks (cond-mat.dis-nn), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Biological sciences, FOS: Biological sciences},
  publisher = {arXiv},
}

@Article{Yamashita2018,
  author     = {Rikiya Yamashita and Mizuho Nishio and Richard Kinh Gian Do and Kaori Togashi},
  journal    = {Insights into Imaging},
  title      = {Convolutional neural networks: an overview and application in radiology},
  year       = {2018},
  month      = {jun},
  number     = {4},
  pages      = {611--629},
  volume     = {9},
  comment    = {-Two key hyperparameters that define the
convolution operation are size and number of kernels. The
former is typically 3 × 3, but sometimes 5 × 5 or 7 × 7
- zeros are added on each side of the input tensor,
so as to fit the center of a kernel on the outermost element and
keep the same in-plane dimension through the convolution
operation

Convolution layer Kernels Kernel size, number of kernels, stride, padding, activation function

Pooling layer None Pooling method, filter size, stride, padding

Fully connected layer Weights Number of weights, activation function

Others Model architecture, optimizer, learning rate, loss function, mini-batch
size, epochs, regularization, weight initialization, dataset splitting

- A max pooling with a filter of size 2 × 2 with a
stride of 2 is commonly used in practice

-Global average pooling typically applied only once before  the fully connected layers

-ctivation function applied to the last fully connected
layer is usually different from the others
-softmax function which normalizes out-
put real values from the last fully connected layer to target
class probabilities

- many improvements on the gra-
dient descent algorithm have been proposed and widely used,
such as SGD with momentum, RMSprop, and Adam

-Dropout is a recently introduced reg-
ularization technique where randomly selected activations are set to 0 during the training, so that the model becomes less
sensitive to specific weights in the network	

-include regularization with dropout or weight decay, batch
normalization, and data augmentation, as well as reducing
architectural complexity

-There are a couple of techniques
available to train a model efficiently on a smaller dataset: data
augmentation and transfer learning.

-whether transfer learning with such networks im-
proves the performance in the medical field compared to that
with ImageNet pretrained models is not clear and remains an
area of further investigation

-Transfere Learning :network is
pretrained on an extremely large dataset, such as ImageNet -> you can also just use a pretrained convolution base for the network that is just capable to recognize the main features 

- the annotation cost for aradiological image is much larger than a general image because radiologist expertise is required for annotation.

- adding a small amount of gaussian noise doesnt impact the generall look of the picture which means it would still be interpreted the same way from a doctor that means that we can use this methode to modify our data},
  doi        = {10.1007/s13244-018-0639-9},
  priority   = {prio1},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank5},
  readstatus = {read},
}

@InCollection{Zeiler2014,
  author    = {Matthew D. Zeiler and Rob Fergus},
  booktitle = {Computer Vision {\textendash} {ECCV} 2014},
  publisher = {Springer International Publishing},
  title     = {Visualizing and Understanding Convolutional Networks},
  year      = {2014},
  pages     = {818--833},
  abstract  = {-},
  doi       = {10.1007/978-3-319-10590-1_53},
}

@Article{Selvaraju2016,
  author     = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title      = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  year       = {2016},
  abstract   = {-proposition of grad cam a technique for producing "visual explanations" for decisions  from a large class of convolutional Neural Networks},
  comment    = {-A drawback of CAM is that it requires feature maps to di-
rectly precede softmax layers, so it is only applicable to a
particular kind of CNN architectures
-We introduce a
new way of combining feature maps using the gradient signal
that does not require any modification in the network architec-
ture

-calc L_GradCAM for class c :
-we first calculate the gradient of the score for that class c,y^c (before the softmax function), with respect to the feature map activation A^k of a convolutional layer i.e. delta y / delta A^k
-these gradients flowing back are global average pooled over the width and height dimensions (indexed by i and j respectively) to obtain the neuron importance wight alpha^c_k

L^c_GradCAM = ReLU(sum_k(alpha^c_k A^k ))

-t Grad-CAM generalizes CAM for a wide
variety of CNN-based architecture},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1610.02391},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio1},
  publisher  = {arXiv},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{Zhou_2016_CVPR,
  author     = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Learning Deep Features for Discriminative Localization},
  year       = {2016},
  month      = {June},
  comment    = {-global average pool-
ing which acts as a structural regularizer, preventing over-
fitting during training
-CAM: . A class activation map for a particular cat-
egory indicates the discriminative image regions used by the
CNN to identify that category

-way easyer to compute that Grad Cam but just works for networks that end with conv and dense layer 

-We found that the localization ability of the networks im-
proved when the last convolutional layer before GAP had a
higher spatial resolution
-CAM enables classification-trained CNNs to learn
to perform object localization, without using any bounding
box annotations},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
}

@InProceedings{pmlr-v48-gal16,
  author     = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle  = {Proceedings of The 33rd International Conference on Machine Learning},
  title      = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  year       = {2016},
  address    = {New York, New York, USA},
  editor     = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month      = {20--22 Jun},
  pages      = {1050--1059},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {48},
  abstract   = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
  comment    = {-Dropout is used in many models in deep
learning as a way to avoid over-fitting
- link between Gaussian processes and dropout
-Recent advances
in variational inference introduced new techniques into
the field such as sampling-based variational inference and
stochastic variational inference
-show that a neural network with arbitrary depth and
non-linearities, with dropout applied before every weight
layer, is mathematically equivalent to an approximation
to the probabilistic deep Gaussian process
-During NN optimisation a regularisation term is
often added. We often use L2 regularisation weighted by
some weight decay λ, resulting in a minimisation objective
-MC dropout can
be approximated by averaging the weights of the network
-) over validation log-likelihood to find optimal τ , and
set the prior length-scale to 10−2 for most datasets based on
the range of the data
- We used dropout with probabilities 0.05 and 0.005
since the network size is very small
-built a probabilistic interpretation of dropout
which allowed us to obtain model uncertainty out of exist-
ing deep learning models},
  pdf        = {http://proceedings.mlr.press/v48/gal16.pdf},
  priority   = {prio1},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v48/gal16.html},
}

@InProceedings{pmlr-v28-wang13a,
  author    = {Wang, Sida and Manning, Christopher},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {Fast dropout training},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  month     = {17--19 Jun},
  number    = {2},
  pages     = {118--126},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective.  This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability.  We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
  pdf       = {http://proceedings.mlr.press/v28/wang13a.pdf},
  url       = {https://proceedings.mlr.press/v28/wang13a.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
