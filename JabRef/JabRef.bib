@Article{Walle2023,
  author     = {Matthias Walle and Dominic Eggemann and Penny R. Atkins and Jack J. Kendall and Kerstin Stock and Ralph Müller and Caitlyn J. Collins},
  journal    = {Bone},
  title      = {Motion grading of high-resolution quantitative computed tomography supported by deep convolutional neural networks},
  year       = {2023},
  month      = {jan},
  pages      = {116607},
  volume     = {166},
  doi        = {10.1016/j.bone.2022.116607},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{Zhang2020,
  author     = {Qiang Zhang and Evan Hann and Konrad Werys and Cody Wu and Iulia Popescu and Elena Lukaschuk and Ahmet Barutcu and Vanessa M. Ferreira and Stefan K. Piechnik},
  journal    = {Artificial Intelligence in Medicine},
  title      = {Deep learning with attention supervision for automated motion artefact detection in quality control of cardiac T1-mapping},
  year       = {2020},
  month      = {nov},
  pages      = {101955},
  volume     = {110},
  doi        = {10.1016/j.artmed.2020.101955},
  publisher  = {Elsevier {BV}},
  readstatus = {skimmed},
}

@Article{Lorch2017,
  author     = {Benedikt Lorch and Ghislain Vaillant and Christian Baumgartner and Wenjia Bai and Daniel Rueckert and Andreas Maier},
  journal    = {Journal of Medical Engineering},
  title      = {Automated Detection of Motion Artefacts in {MR} Imaging Using Decision Forests},
  year       = {2017},
  month      = {jun},
  pages      = {1--9},
  volume     = {2017},
  doi        = {10.1155/2017/4501647},
  publisher  = {Hindawi Limited},
  readstatus = {skimmed},
}

@Article{Alzubaidi2021,
  author     = {Laith Alzubaidi and Jinglan Zhang and Amjad J. Humaidi and Ayad Al-Dujaili and Ye Duan and Omran Al-Shamma and J. Santamar{\'{\i}}a and Mohammed A. Fadhel and Muthana Al-Amidie and Laith Farhan},
  journal    = {Journal of Big Data},
  title      = {Review of deep learning: concepts, {CNN} architectures, challenges, applications, future directions},
  year       = {2021},
  month      = {mar},
  number     = {1},
  volume     = {8},
  comment    = {Optimizer selection:
- two major issues in the learning process are learning algorithm selection(optimizer) while the other one is the issue of many enhancements(AdaDelta, Adagrad, and momentum)
- batch gradient descent: the network
parameters are updated merely one time behind considering all training datasets via
the network
-Stochastic Gradient Descent: The parameters are updated at each training sample
Mini-batch Gradient Descent: In this approach, the training samples are partitioned
into several mini-batches
-other methodes are Momentum and Adaptive Moment Estimation
-AlexNet improved its learning ability by its depth and implementing several parameter optimization strategies-> number extraction stages was increased
- ZefNet was the frontier network, which proposed that filters with small sizes
could enhance the CNN performance
-VGG inserted a
layer of the heap of 3 × 3 filters rather than the 5 × 5 and 11 × 11 filters in ZefNet.
-GoogLeNet combines multiple-scale convolutional transformations by employing merge, transform, and split functions for feature extraction
-architecture incorporates filters Page 31 of 74Alzubaidi et al. J Big Data (2021) 8:53
of different sizes (5 × 5, 3 × 3, and 1 × 1) to capture channel information together with
spatial information at diverse ranges of spatial resolution
-proposed the idea of aux-
iliary learners to speed up the rate of convergence
-The novel idea of ResNet is its use of the
bypass pathway concept
-ayer widening is a highly successful method of per-
formance enhancement compared to deepening the residual network.
-slow enlargement in the feature map depth based on the
up-down method
-Xception model
adjusted the original inception block by making it wider and exchanging a single
dimension (3 × 3) followed by a 1 × 1 convolution to reduce computational complex-
ity

-> high resolution network might be a good approach for my thesis (necessary for position-sensitive vision tasks)

- TL as compared to the target dataset. For instance,
enhancing the medical image classification performance of CNN models is achieved
by training the models using the ImageNet dataset, which contains natural images

-TL from different domains does not significantly affect performance on
medical imaging tasks

-ncrease the amount of available data and avoid the overfitting issue, data
augmentation techniques are one possible solution : Flipping, Cropping 
Rotation Translation

- biological data tends to be imbalanced, as negative samples are much more numerous than positive ones, care for this before training because the model might not perform well on small classes 
-in healthcare or similar
applications, the uncertainty scaling is frequently very significant

-paper has greate information about how to tackle the vanishing gradient problem 
-underspecification: It has been shown that small modifica-
tions can force a model towards a completely different solution as well as lead to dif-
ferent predictions in deployment domains

Paper contains many intresting evaluation techniques like :Accuracy Sensitivity Specificity, Precision, F1_Score, J_Score, False Positive Rate FPR, Area Under the ROC Curve},
  doi        = {10.1186/s40537-021-00444-8},
  priority   = {prio1},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{LeCun2015,
  author    = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  journal   = {Nature},
  title     = {Deep learning},
  year      = {2015},
  month     = {may},
  number    = {7553},
  pages     = {436--444},
  volume    = {521},
  doi       = {10.1038/nature14539},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Khan2020,
  author    = {Asifullah Khan and Anabia Sohail and Umme Zahoora and Aqsa Saeed Qureshi},
  journal   = {Artificial Intelligence Review},
  title     = {A survey of the recent architectures of deep convolutional neural networks},
  year      = {2020},
  month     = {apr},
  number    = {8},
  pages     = {5455--5516},
  volume    = {53},
  doi       = {10.1007/s10462-020-09825-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{Ruder2016,
  author    = {Ruder, Sebastian},
  title     = {An overview of gradient descent optimization algorithms},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1609.04747},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InProceedings{Zhang2018,
  author    = {Zijun Zhang},
  booktitle = {2018 {IEEE}/{ACM} 26th International Symposium on Quality of Service ({IWQoS})},
  title     = {Improved Adam Optimizer for Deep Neural Networks},
  year      = {2018},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/iwqos.2018.8624183},
}

@Misc{Lin2013,
  author     = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  title      = {Network In Network},
  year       = {2013},
  abstract   = {-Introduces the Network in Network structure to enhance model discriminability for local patches
-we build micro neural networks with
more complex structures to abstract the data within the receptive field
-we are able to uti-
lize global average pooling over feature maps in the classification layer, which is
easier to interpret and less prone to overfitting than traditional fully connected lay-
er},
  comment    = {For Code: https://github.com/ducha-aiki/caffenet-benchmark

-generalized linear model(GLM)
- In NIN, the
GLM is replaced with a ”micro network” structure which is a general nonlinear function approxi-
mator
-a micro network is
introduced within each convolutional layer to compute more abstract features for local patches

-Each pooling layer performs
weighted linear recombination on the input feature maps, which then go through a rectifier linear
unit. The cross channel pooled feature maps are cross channel pooled again and again in the next
layers. This cascaded cross channel parameteric pooling structure allows complex and learnable
interactions of cross channel information
-for classification tasks},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1312.4400},
  keywords   = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio2},
  publisher  = {arXiv},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Mishkin2017,
  author     = {Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas},
  journal    = {Computer Vision and Image Understanding},
  title      = {Systematic evaluation of convolution neural network advances on the Imagenet},
  year       = {2017},
  month      = {aug},
  pages      = {11--19},
  volume     = {161},
  abstract   = {-studies the impact of a range of recent advances in
CNN architectures and learning method},
  comment    = {- ELU + maxout hsow the best performance among non-linearities with speed close to ReLu

- Wide maxout outperforms the rest of the competitors at
a higher computational cost
-Pooling, combined with striding, is a common way to archive a degree of in-
variance together with a reduction of spatial size of feature map

Prediction:  max pooling brings selectivity and in-
variance, while average pooling allows using gradients of all filters, instead of
throwing away 3/4 of information 
- most commonly used learn-
ing rate decay policy is ”reduce learning rate 10x, when validation error stops
decreasing
-Batch Normalization is a recent method that solves the gradient ex-
ploding/vanishing problem and guarantees near-optimal learning regim


Conclusion:
• use ELU non-linearity without batchnorm or ReLU with it.
• apply a learned colorspace transformation of RGB.
• use the linear learning rate decay policy.
• use a sum of the average and max pooling layers.
• use mini-batch size around 128 or 256. If this is too big for your GPU,
decrease the learning rate proportionally to the batch size.
• use fully-connected layers as convolutional and average the predictions for
the final decision.
• when investing in increasing training set size, check if a plateau has not
been reach.
• cleanliness of the data is more important then the size.
• if you cannot increase the input image size, reduce the stride in the con-
sequent layers, it has roughly the same effect.
• if your network has a complex and highly optimized architecture, like e.g.
GoogLeNet, be careful with modifications},
  doi        = {10.1016/j.cviu.2017.05.007},
  priority   = {prio1},
  publisher  = {Elsevier {BV}},
  ranking    = {rank4},
  readstatus = {read},
}

@InCollection{Bottou2010,
  author    = {L{\'{e}}on Bottou},
  booktitle = {Proceedings of {COMPSTAT}{\textquotesingle}2010},
  publisher = {Physica-Verlag {HD}},
  title     = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  year      = {2010},
  pages     = {177--186},
  doi       = {10.1007/978-3-7908-2604-3_16},
}

@Misc{Simonyan2014,
  author    = {Simonyan, Karen and Zisserman, Andrew},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1409.1556},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Misc{Dauphin2015,
  author    = {Dauphin, Yann N. and de Vries, Harm and Bengio, Yoshua},
  title     = {Equilibrated adaptive learning rates for non-convex optimization},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1502.04390},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Misc{Woo2018,
  author     = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  title      = {CBAM: Convolutional Block Attention Module},
  year       = {2018},
  abstract   = {a simple yet effective attention module for feed-forward convolutional
neural networks},
  comment    = {esearches have mainly investigated three
important factors of networks: depth, width, and cardinality
-The overview of CBAM: The module has two sequential sub-modules:
channel and spatial. The intermediate feature map is adaptively refined through
our module (CBAM) at every convolutional block of deep networks
-An intuitive and simple way of extension is to increase the depth of neural
networks
-naive increase in depth comes to saturation due to the difficulty of gradient
propagation
-introduces  cannel attention and spatial attention model 
-One can seamlessly integrate CBAM in any CNN architectures and jointly train the combined
CBAM-enhanced networks
-CBAM outperforms all the
Convolutional Block Attention Module 7
baselines without bells and whistles, demonstrating the general applicability of
CBAM across different architectures as well as different tasks.
-Using both
attention is critical while the best-combining strategy is Network+ channel + spatial
- Grad Cam shows that the use of CBAM in a network gives it a better better and more precise way of detecting objects 
-improve representation power of CNN networks

-},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1807.06521},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority   = {prio1},
  publisher  = {arXiv},
  ranking    = {rank5},
  readstatus = {read},
}

@InProceedings{He_2016_CVPR,
  author     = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Deep Residual Learning for Image Recognition},
  year       = {2016},
  month      = {June},
  abstract   = {-present a residual learning framework to ease the training
of networks that are substantially deeper than those used
previously

won the 1st place on the
ILSVRC 2015 classification task},
  comment    = {Is
learning better networks as easy as stacking more layers?

-Multigrid method reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale
-An early practice of training multi-layer perceptrons
(MLPs) is to add a linear layer connected from the network
input to the output
-y = F(x, {Wi}) + Wsx.},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
}

@Misc{Hu2017,
  author    = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  title     = {Squeeze-and-Excitation Networks},
  year      = {2017},
  abstract  = {We further demonstrate
that Squeeze and Excitation blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cos

https://github.com/hujie-frank/SENet},
  comment   = {-we propose a mechanism
that allows the network to perform feature recalibration,
through which it can learn to use global information to
selectively emphasise informative features and suppress less
useful ones
-ResNets demonstrated that it was possible to learn considerably deeper and stronger networks
through the use of identity-based skip connections},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1709.01507},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InCollection{Tan2018,
  author    = {Chuanqi Tan and Fuchun Sun and Tao Kong and Wenchang Zhang and Chao Yang and Chunfang Liu},
  booktitle = {Artificial Neural Networks and Machine Learning {\textendash} {ICANN} 2018},
  publisher = {Springer International Publishing},
  title     = {A Survey on Deep Transfer Learning},
  year      = {2018},
  pages     = {270--279},
  doi       = {10.1007/978-3-030-01424-7_27},
}

@InProceedings{NEURIPS2019_eb1e7832,
  author    = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Transfusion: Understanding Transfer Learning for Medical Imaging},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
}

@Article{Herzog2020,
  author    = {Lisa Herzog and Elvis Murina and Oliver Dürr and Susanne Wegener and Beate Sick},
  journal   = {Medical Image Analysis},
  title     = {Integrating uncertainty in deep neural networks for {MRI} based stroke analysis},
  year      = {2020},
  month     = {oct},
  pages     = {101790},
  volume    = {65},
  doi       = {10.1016/j.media.2020.101790},
  publisher = {Elsevier {BV}},
}

@Article{Pereyra2017,
  author        = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Łukasz and Hinton, Geoffrey},
  title         = {Regularizing Neural Networks by Penalizing Confident Output Distributions},
  year          = {2017},
  month         = jan,
  abstract      = {We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1701.06548},
  eprint        = {1701.06548},
  file          = {:http\://arxiv.org/pdf/1701.06548v1:PDF},
  keywords      = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.NE},
  publisher     = {arXiv},
}

@Article{Han2015,
  author        = {Han, Song and Mao, Huizi and Dally, William J.},
  title         = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  year          = {2015},
  month         = oct,
  abstract      = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1510.00149},
  eprint        = {1510.00149},
  file          = {:http\://arxiv.org/pdf/1510.00149v5:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{DAmour2020,
  author        = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  title         = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  year          = {2020},
  month         = nov,
  abstract      = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2011.03395},
  eprint        = {2011.03395},
  file          = {:http\://arxiv.org/pdf/2011.03395v2:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{pmlr-v28-goodfellow13,
  author    = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {Maxout Networks},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  month     = {17--19 Jun},
  number    = {3},
  pages     = {1319--1327},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
  pdf       = {http://proceedings.mlr.press/v28/goodfellow13.pdf},
  url       = {https://proceedings.mlr.press/v28/goodfellow13.html},
}

@Misc{Mishkin2015,
  author    = {Mishkin, Dmytro and Matas, Jiri},
  title     = {All you need is a good init},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1511.06422},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Book{Briggs2000,
  author    = {Briggs, William L.},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {A multigrid tutorial.},
  year      = {2000},
  isbn      = {0898714621},
  pages     = {193},
}

@Misc{Kingma2014,
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1412.6980},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InCollection{He2016,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle = {Computer Vision {\textendash} {ECCV} 2016},
  publisher = {Springer International Publishing},
  title     = {Identity Mappings in Deep Residual Networks},
  year      = {2016},
  pages     = {630--645},
  doi       = {10.1007/978-3-319-46493-0_38},
}

@Comment{jabref-meta: databaseType:bibtex;}
